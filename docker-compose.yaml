version: '2.4'

services:
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      PYTHONUNBUFFERED: 1
      REDIS_HOST: redis
      REDIS_PORT: 6379
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY} 
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
      S3_BUCKET: ${S3_BUCKET}
      AWS_REGION: ${AWS_REGION}
      AWS_ENDPOINT_URL: http://localstack:4566
      MONGO_URI: mongodb://root:example@mongo:27017
      SEARCH_SERVICE_HOST: http://search-service:5000
      TRAINING_SERVICE_HOST: http://training-service:5001
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - mongo
      - redis
      - search-service
      - training-service
    networks:
      - app-network

  ml-base:
    build:
      context: ./trainer
      dockerfile: Dockerfile.base
    image: ml-base:latest

  search-service:
    build:
      context: ./trainer
      dockerfile: Dockerfile.python
      args:
        PYTHON_VERSION: 3.9
        BUILD_TYPE: gpu
        PYTORCH_INDEX: "https://download.pytorch.org/whl/cu118"
        BASE_IMAGE: ml-base:latest
    image: search-service:latest
    runtime: nvidia
    environment:
      PYTHONUNBUFFERED: 1
      REDIS_HOST: redis
      REDIS_PORT: 6379
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY}
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
      S3_BUCKET: ${S3_BUCKET}
      AWS_REGION: ${AWS_REGION}
      AWS_ENDPOINT_URL: http://localstack:4566
      SERVICE_PORT: 5000
      NVIDIA_VISIBLE_DEVICES: all
      CUDA_VISIBLE_DEVICES: 0
      MODEL_CACHE_DIR: /app/model_cache
      SHARED_MODELS_DIR: /app/shared_models
      TRANSFORMER_CACHE: /app/model_cache/transformers
      HF_HOME: /app/model_cache/huggingface
      TORCH_HOME: /app/model_cache
    ports:
      - "5000:5000"
    command: ["python", "search_service.py"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - redis
      - ml-base
    networks:
      - app-network
    volumes:
      - model_cache:/app/model_cache:rw
      - shared_models:/app/shared_models:rw
    user: "1000:1000"  # Add this line to match your host user ID
    entrypoint: >
      /bin/sh -c "
      python search_service.py
      "

  training-service:
    build:
      context: ./trainer
      dockerfile: Dockerfile.python
      args:
        PYTHON_VERSION: 3.9
        BUILD_TYPE: gpu
        PYTORCH_INDEX: "https://download.pytorch.org/whl/cu118"
        BASE_IMAGE: ml-base:latest
    image: training-service:latest
    runtime: nvidia
    environment:
      PYTHONUNBUFFERED: 1
      REDIS_HOST: redis
      REDIS_PORT: 6379
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY}
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
      S3_BUCKET: ${S3_BUCKET}
      AWS_REGION: ${AWS_REGION}
      AWS_ENDPOINT_URL: http://localstack:4566
      SERVICE_PORT: 5001
      TRAINING_QUEUE: training_queue
      MODEL_PREFIX: model_status
      NVIDIA_VISIBLE_DEVICES: all
      CUDA_VISIBLE_DEVICES: 0
      MODEL_CACHE_DIR: /app/model_cache
      SHARED_MODELS_DIR: /app/shared_models
      TRANSFORMER_CACHE: /app/model_cache/transformers
      HF_HOME: /app/model_cache/huggingface
      TORCH_HOME: /app/model_cache
      TRANSFORMERS_OFFLINE: "0"  # Ensure we can download models
      HF_DATASETS_OFFLINE: "0"
      TRANSFORMERS_CACHE: /app/model_cache/transformers
    ports:
      - "5001:5001"
    command: ["python", "training_service.py"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - redis
      - ml-base
    networks:
      - app-network
    volumes:
      - model_cache:/app/model_cache:rw
      - shared_models:/app/shared_models:rw
      - ./cache/temp:/tmp/training:rw  # Add temp storage
    tmpfs:
      - /tmp:exec,size=4G  # Add more temp space
    user: "1000:1000"  # Add this line to match your host user ID
    entrypoint: >
      /bin/sh -c "
      python training_service.py
      "

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - app-network

  mongo:
    image: mongo:latest
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
    volumes:
      - mongodb_data:/data/db
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - app-network

  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3
      - AWS_DEFAULT_REGION=us-east-1
      - EDGE_PORT=4566
      - DEBUG=1
      - PERSISTENCE=1
      - DATA_DIR=/var/lib/localstack/data
      - DOCKER_HOST=unix:///var/run/docker.sock
      - LEGACY_DIRECTORIES=1
      - DISABLE_CORS_CHECKS=1
      # Add these environment variables
      - LOCALSTACK_VOLUME_DIR=/var/lib/localstack
      - FORCE_NONINTERACTIVE=1
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "./localstack/data:/var/lib/localstack/data"  # Changed this
      - "./localstack/init-s3.sh:/docker-entrypoint-initaws.d/init-s3.sh"
      # Remove the bind mount causing issues and replace with this
      - "./cache/s3:/var/lib/localstack/s3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - app-network

volumes:
  mongodb_data:
  redis_data:
  localstack_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/localstack/data
      uid: "1000"
      gid: "1000"
      mode: "0755"
  model_cache:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${PWD}/cache/models  # Create this directory in your project
      uid: "1000"  # Add this
      gid: "1000"  # Add this
      mode: "0755" # Add this
  shared_models:
    driver: local

networks:
  app-network:
