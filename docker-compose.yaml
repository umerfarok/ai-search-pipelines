version: '2.3'

services:
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    ports:
       - "0.0.0.0:8080:8080"
    environment:
      PYTHONUNBUFFERED: 1
      REDIS_HOST: redis
      REDIS_PORT: 6379
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY} 
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
      S3_BUCKET: ${S3_BUCKET}
      AWS_REGION: ${AWS_REGION}
      AWS_ENDPOINT_URL: http://localstack:4566
      MONGO_URI: mongodb://root:example@mongo:27017
      SEARCH_SERVICE_HOST: http://search-service:5000
      TRAINING_SERVICE_HOST: http://training-service:5001
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - mongo
      - redis
      - search-service
      - training-service
    networks:
      - app-network

  ml-base:
    build:
      context: ./trainer
      dockerfile: Dockerfile.base
    image: ml-base:latest

  search-service:
    build:
      context: ./trainer
      dockerfile: Dockerfile.python
      args:
        PYTHON_VERSION: 3.9
        BUILD_TYPE: gpu
        PYTORCH_INDEX: "https://download.pytorch.org/whl/cu118"
        BASE_IMAGE: ml-base:latest
    image: search-service:latest
    environment:
      PYTHONUNBUFFERED: 1
      REDIS_HOST: redis
      REDIS_PORT: 6379
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY}
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
      S3_BUCKET: ${S3_BUCKET}
      AWS_REGION: ${AWS_REGION}
      AWS_ENDPOINT_URL: http://localstack:4566
      SERVICE_PORT: 5000
      NVIDIA_VISIBLE_DEVICES: all
      CUDA_VISIBLE_DEVICES: "0"  # Enable CUDA
      FORCE_CPU: "0"  # Allow GPU usage
      MODEL_CACHE_DIR: /app/models/cache  # Cache within shared directory
      SHARED_MODELS_DIR: /app/models  # Simplified shared path
      TRANSFORMER_CACHE: /app/model_cache/transformers
      HF_HOME: /app/model_cache/huggingface
      TORCH_HOME: /app/model_cache
      CUDA_LAUNCH_BLOCKING: "1"  # For better error reporting
      TORCH_USE_CUDA_DSA: "1"    # Enable device-side assertions
      PYTORCH_NO_CUDA_MEMORY_CACHING: "1"  # Add this
      S3_CACHE_DIR: /app/s3_cache
      TEMP_DIR: /tmp/search
      CUDA_DEVICE_MAX_CONNECTIONS: "1"
      NCCL_P2P_DISABLE: "1"
      MODEL_LOAD_DIR: /app/models/models  # Add explicit load directory
      TRANSFORMERS_CACHE: "/app/model_cache/transformers"
      XDG_CACHE_HOME: "/app/model_cache"
    ports:
      - "5000:5000"
    command: ["python", "search_service.py"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on: 
      - redis
      - ml-base
    networks:
      - app-network
    volumes:
      - type: bind
        source: ./models
        target: /app/models
        consistency: cached
      - type: bind
        source: ./cache/temp
        target: /tmp/search
      - type: bind
        source: ./cache/s3_cache
        target: /app/s3_cache
      - type: bind
        source: ./cache/model_cache
        target: /app/model_cache
        consistency: cached
    user: "${UID:-1000}:${GID:-1000}"  # Explicit user mapping
    entrypoint: >
      /bin/sh -c "
      python search_service.py
      "
    deploy:  # Add deploy configuration
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    runtime: nvidia  # Add this back

  training-service:
    build:
      context: ./trainer
      dockerfile: Dockerfile.python
      args:
        PYTHON_VERSION: 3.9
        BUILD_TYPE: gpu
        PYTORCH_INDEX: "https://download.pytorch.org/whl/cu118"
        BASE_IMAGE: ml-base:latest
    image: training-service:latest
    environment:
      PYTHONUNBUFFERED: 1
      REDIS_HOST: redis
      REDIS_PORT: 6379
      AWS_ACCESS_KEY: ${AWS_ACCESS_KEY}
      AWS_SECRET_KEY: ${AWS_SECRET_KEY}
      S3_BUCKET: ${S3_BUCKET}
      AWS_REGION: ${AWS_REGION}
      AWS_ENDPOINT_URL: http://localstack:4566
      SERVICE_PORT: 5001
      TRAINING_QUEUE: training_queue
      MODEL_PREFIX: model_status
      NVIDIA_VISIBLE_DEVICES: all
      CUDA_VISIBLE_DEVICES: "0"  # Enable CUDA
      FORCE_CPU: "0"  # Allow GPU usage
      MODEL_CACHE_DIR: /app/models/cache  # Same cache location
      SHARED_MODELS_DIR: /app/models  # Same shared path
      TRANSFORMER_CACHE: /app/model_cache/transformers
      HF_HOME: /app/model_cache/huggingface
      TORCH_HOME: /app/model_cache
      TRANSFORMERS_OFFLINE: "0"  # Ensure we can download models
      HF_DATASETS_OFFLINE: "0"
      TRANSFORMERS_CACHE: /app/model_cache/transformers
      CUDA_LAUNCH_BLOCKING: "1"  # For better error reporting
      TORCH_USE_CUDA_DSA: "1"    # Enable device-side assertions
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"  # Help prevent memory fragmentation
      MODEL_SAVE_DIR: /app/models/models  # Add explicit save directory
      XDG_CACHE_HOME: "/app/model_cache"
    ports:
      - "5001:5001"
    command: ["python", "training_service.py"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - redis
      - ml-base
    networks:
      - app-network
    volumes:
      - type: bind
        source: ./models
        target: /app/models
        consistency: cached
      - type: bind
        source: ./cache/temp
        target: /tmp/training
      - type: bind
        source: ./cache/model_cache
        target: /app/model_cache
        consistency: cached
    tmpfs:
      - /tmp:exec,size=4G  # Add more temp space
    user: "${UID:-1000}:${GID:-1000}"  # Explicit user mapping
    entrypoint: >
      /bin/sh -c "
      python training_service.py
      "
    deploy: 
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    runtime: nvidia  

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
     
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - app-network

  mongo:
    image: mongo:latest
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
    volumes:
      - mongodb_data:/data/db
    healthcheck:
      test: ["CMD", "echo", "db.runCommand('ping').ok", "|", "mongosh", "localhost:27017/test", "--quiet"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - app-network

  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566"
    environment:
      - SERVICES=s3
      - AWS_DEFAULT_REGION=us-east-1
      - EDGE_PORT=4566
      - DEBUG=1
      - PERSISTENCE=1
      - DATA_DIR=/var/lib/localstack/data
      - DOCKER_HOST=unix:///var/run/docker.sock
      - LEGACY_DIRECTORIES=1
      - DISABLE_CORS_CHECKS=1
      - LOCALSTACK_VOLUME_DIR=/var/lib/localstack
      - FORCE_NONINTERACTIVE=1
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      - "./localstack/data:/var/lib/localstack/data"
      - "./localstack/init-s3.sh:/docker-entrypoint-initaws.d/init-s3.sh"
      - "./cache/s3:/var/lib/localstack/s3"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4566/_localstack/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - app-network
  frontend:
    build:
      context: ./playground 
      dockerfile: Dockerfile
    ports:
      - "0.0.0.0:3000:3000"
    environment:
      - NEXT_PUBLIC_API_URL=http://api:8080
    networks:
      - app-network
    depends_on:
      - api
    volumes:
      - type: bind
        source: ./models
        target: /app/models
        consistency: cached
      - type: bind
        source: ./cache/model_cache
        target: /app/model_cache
        consistency: cached
      - type: bind
        source: ./cache/temp
        target: /tmp/search
      - type: bind
        source: ./cache/s3_cache
        target: /app/s3_cache
networks:
  app-network:

volumes:
  models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./models
      uid: "${UID:-1000}"
      gid: "${GID:-1000}"
  mongodb_data:
    driver: local
  redis_data:
    driver: local
  localstack_data:
    driver: local
  model_cache:
    driver: local